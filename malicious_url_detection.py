# -*- coding: utf-8 -*-
"""Malicious URL Detection.ipynb

Automatically generated by Colaboratory.

   
"""

# Heemani Brahmbhatt
# Malicious URL detection - Classification Project
'''
Project Description: This project uses a labeled database of URLs to help predict if a given URL is malicious or not. 
Furthermore, the malicious URLs can be classified into one of the following types of malicious intent: Phishing, Defacement, Malware
'''

import pandas as pd

df = pd.read_csv("malicious_phish.csv")
df.head() # view the first 5 rows as a sanity check

"""# **Exploratory Data Analysis**"""

df.info()

df.shape

b = df['type'].value_counts() # gets the individual counts of the instances of each class

print(b)

import matplotlib.pyplot as plt
import numpy as np

print(b.benign), print(b.defacement), print(b.phishing), print(b.malware)

classes = np.array(['benign', 'defacement', 'phishing', 'malware'])
y = [b.benign, b.defacement, b.phishing, b.malware]

plt.pie(y, labels=classes)
plt.show() # shows the class distibution of the data

"""We can see the class imbalance here. While there is an abundance of instances of the *benign* class, we have very few samples from the other three class. In fact, all of the malicious classes combined still do not equal in quantity the benign class. The smallest class is the malware class, thus when we're performing multiclass classification, we can expect the evaluation metrics for the malware class to be relatively low. This can be addressed by oversampling methods."""

df['type'].count()

# To get the precise percentages of the class splits, we can do some more math
b_percent = b.benign/df['type'].count()
d_percent = b.defacement/df['type'].count()
p_percent = b.phishing/df['type'].count()
m_percent = b.malware/df['type'].count()

print("Benign: {:.2f}%, Defacement: {:.2f}%, Phishing: {:.2f}%, Malware: {:.2f}%".format(b_percent,d_percent,p_percent,m_percent))

"""Now it's clear that the majority of the data is 'benign' class (66%) while the other 34% is malicious URLs which are then subdivided by type.

# **Classification**

## **Binary Classification**

For binary classification, we simply focus on the task of determining whether a given URL is malicious or not.
"""

df2 = df

df2

df2 = df2.replace("phishing", "malicious")
df2 = df2.replace("defacement", "malicious")
df2 = df2.replace("malware", "malicious")

df2

"""Now all malicious instances are labeled as malicious while the benign URLs remain benign. Now we can use a few classifcation techniques to decipher one type from another.

## Naive Bayes Classification
"""

sampledf = df2.sample(frac=0.01)

sampledf

# We start by splitting our data into test/train sets
url = np.array(sampledf.url.values)
label = np.array(sampledf.type.values)

from sklearn.model_selection import train_test_split

train_url, test_url, train_label, test_label = train_test_split(url, label, test_size=0.2)

from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import CountVectorizer


vectorizer = CountVectorizer()
vectorizer.fit(train_url)

"""Now we have our data (from the df2 dataframe) split into training and testing data. 80% of the data will be used for training, while 20% will be used for testing. This split is approporiate for the set since there are so many data instances."""

from sklearn.naive_bayes import GaussianNB

X_train = vectorizer.transform(train_url)
X_test = vectorizer.transform(test_url)
#y_train = train_label

classifier = GaussianNB() # instantiate a Gaussian Naive Bayes classifier

classifier.fit(X_train.todense(), train_label)

y_pred = classifier.predict(X_test.todense())

y_pred

test_label

from sklearn import metrics
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

nb_accuracy = metrics.accuracy_score(test_label, y_pred) # accuracy
nb_error = 1 - metrics.accuracy_score(test_label, y_pred) # error
nb_precision = metrics.precision_score(test_label, y_pred, average = None) # precision
nb_recall = metrics.recall_score(test_label, y_pred, average = None) # recall
nb_f1_score = "F1 Score",metrics.f1_score(test_label, y_pred, average = None) # F1 score

print("\nNAIVE BAYES RESULTS")
print("Accuracy", nb_accuracy) # accuracy
print("Error",nb_error) # error
print("Precision", nb_precision) # precision
print("Recall", nb_recall) # recall
print("F1 Score", nb_f1_score) # F1 score

"""## Logistic Regression Classification"""

from sklearn.linear_model import LogisticRegression

LR_classifier = LogisticRegression(multi_class='ovr') # for binary classification

LR_classifier.fit(X_train, train_label)

y_pred = LR_classifier.predict(X_test)

lr_accuracy = metrics.accuracy_score(test_label, y_pred) # accuracy
lr_error = 1 - metrics.accuracy_score(test_label, y_pred) # error
lr_precision = metrics.precision_score(test_label, y_pred, average = None) # precision
lr_recall = metrics.recall_score(test_label, y_pred, average = None) # recall
lr_f1_score = "F1 Score",metrics.f1_score(test_label, y_pred, average = None) # F1 score

print("\nLOGISTIC REGRESSION RESULTS")
print("Accuracy", lr_accuracy) # accuracy
print("Error", lr_error) # error
print("Precision", lr_precision) # precision
print("Recall", lr_recall) # recall
print("F1 Score", lr_f1_score) # F1 score

"""## Support Vector Machine Classification"""

from sklearn.svm import SVC

SVM_classifier = SVC() # kernel is set to rbf by default

SVM_classifier.fit(X_train, train_label)

y_pred = SVM_classifier.predict(X_test)

svm_accuracy = metrics.accuracy_score(test_label, y_pred) # accuracy
svm_error = 1 - metrics.accuracy_score(test_label, y_pred) # error
svm_precision = metrics.precision_score(test_label, y_pred, average = None) # precision
svm_recall = metrics.recall_score(test_label, y_pred, average = None) # recall
svm_f1_score = "F1 Score",metrics.f1_score(test_label, y_pred, average = None) # F1 score

print("\nSVM RESULTS")
print("Accuracy", svm_accuracy) # accuracy
print("Error", svm_error) # error
print("Precision", svm_precision) # precision
print("Recall", svm_recall) # recall
print("F1 Score", svm_f1_score) # F1 score

"""So in terms of the Binary Classification problem of determining whether a given URL is benign or malicious, it appears that the best classifier is the Logistic Regression Classifier when we take into account the Precision, Recall and F1 score for each class.

# **Multiclass Classification**

### Multinomial Naive Bayes
"""

